{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install fluidsynth -y\n",
    "!pip install midi2audio\n",
    "!pip install music21\n",
    "!pip install pedalboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import corpus, converter, instrument, note, stream, chord, duration, tempo\n",
    "from keras.layers import LSTM, Input, Dropout, Dense, Activation, Embedding, Concatenate, Reshape\n",
    "from keras.layers import Flatten, RepeatVector, Permute, TimeDistributed\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.layers import Multiply, Lambda, Softmax\n",
    "import keras.backend as K \n",
    "from keras.models import Model\n",
    "from IPython.display import Image, Audio\n",
    "from midi2audio import FluidSynth\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "import keras\n",
    "import time\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8195b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_files = [ele for ele in glob.glob(\"../input/classical-music-midi/*/*\")]\n",
    "print(\"A random song file example: \", random.sample(music_files, 1))\n",
    "print(\"Number of songs are:\", len(music_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a53134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_chord_durations(number):\n",
    "    if(number>1.25):\n",
    "        return 2\n",
    "    if(number>.30):\n",
    "        return 1.25\n",
    "    if(number>.10):\n",
    "        return 0.3\n",
    "    return 0.1\n",
    "\n",
    "def get_processed_chords_from_music_files(music_files):\n",
    "    chords = []\n",
    "    for file_number, filename in enumerate(music_files):\n",
    "        chords.append(converter.parse(filename).chordify())\n",
    "        if(file_number%10==0):\n",
    "            print(f\"Completed processing of {round(file_number/len(music_files)*100, 2)}% of files\")\n",
    "    \n",
    "    processed_chords = []\n",
    "    for i, chord_metadata in enumerate(chords):\n",
    "\n",
    "        for element in chord_metadata.flat:\n",
    "            if isinstance(element, chord.Chord):\n",
    "                chord_duration = round_chord_durations(element.duration.quarterLength)\n",
    "                chord_name = \".\".join([n.nameWithOctave for n in element.pitches])\n",
    "                processed_chords.append((chord_name, chord_duration))\n",
    "\n",
    "    pickle.dump(processed_chords, open(\"chords.data\", \"wb\"))\n",
    "    return processed_chords\n",
    "\n",
    "try:\n",
    "    open(\"chords.data\", \"rb\")\n",
    "    processed_chords = pickle.load(open(\"chords.data\", \"rb\"))\n",
    "except:\n",
    "    processed_chords = get_processed_chords_from_music_files(music_files)\n",
    "    \n",
    "print(\"The first 20 processed chords are:\", processed_chords[:20])\n",
    "print(\"Total number of chords played in our dataset:\", len(processed_chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bdb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_music_midi_filename_from_chords(input_chords):\n",
    "    midi_stream = stream.Stream()\n",
    "\n",
    "    for note_pattern, duration_pattern in input_chords:\n",
    "        notes_in_chord = note_pattern.split('.')\n",
    "        \n",
    "        chord_notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(current_note)\n",
    "            new_note.duration = duration.Duration(duration_pattern)\n",
    "            new_note.storedInstrument = instrument.Violoncello()\n",
    "            chord_notes.append(new_note)\n",
    "        new_chord = chord.Chord(chord_notes)\n",
    "        \n",
    "        midi_stream.append(new_chord)\n",
    "\n",
    "        new_tempo = tempo.MetronomeMark(number=50)\n",
    "            \n",
    "        midi_stream.append(new_tempo)\n",
    "\n",
    "    midi_stream = midi_stream.chordify()\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    new_file = 'output-' + timestr + '.mid'\n",
    "    return midi_stream.write('midi', fp=new_file)\n",
    "\n",
    "print(\"Generating music from our processed chords...\")\n",
    "proccessed_chords_to_midi_sample = get_music_midi_filename_from_chords(processed_chords[210:230])\n",
    "FluidSynth().midi_to_audio(proccessed_chords_to_midi_sample, \"music.wav\")\n",
    "\n",
    "Audio(\"music.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ea4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_summary(chord_data):\n",
    "    count = collections.Counter(chord_data)\n",
    "    print(\"Total number of chords in data:\", len(chord_data))\n",
    "    print(\"Unique types of chords in data:\", len(count))\n",
    "    print(\"Count of most common chord:\", max(count.values()))\n",
    "    print(\"Number of chords that only occur once:\", collections.Counter(count.values())[1])\n",
    "    print(\"5 random samples from dataset\", random.sample(chord_data, 5))\n",
    "\n",
    "data_summary(processed_chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451150cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_data(data):\n",
    "    simplify_chord_data = []\n",
    "    for chord_name, chord_duration in data:\n",
    "        cleaned_chord = \"\"\n",
    "\n",
    "        if(\"-\" in chord_name):\n",
    "            chord_name = chord_name.replace(\"-\", \"\")\n",
    "\n",
    "        split_chord = chord_name.split(\".\")\n",
    "        split_chord.sort(reverse=True)\n",
    "\n",
    "        for split in split_chord:\n",
    "\n",
    "            processed_note = split[0] # A,B,C etc.\n",
    "\n",
    "            if(processed_note not in cleaned_chord): # choose only one note of each type in a chord\n",
    "                if(split[-1] in \"12\"):\n",
    "                    processed_note += \"2\"\n",
    "                else:\n",
    "                    processed_note += split[-1]\n",
    "                cleaned_chord += processed_note+\".\"\n",
    "\n",
    "        if(\".\" == cleaned_chord[-1]):\n",
    "            cleaned_chord = cleaned_chord[:len(cleaned_chord)-1]\n",
    "\n",
    "        simplify_chord_data.append((cleaned_chord, chord_duration))\n",
    "    \n",
    "    return simplify_chord_data\n",
    "    \n",
    "print(\"-\"*80)\n",
    "print(\"Example ouput for simplify_data()\")\n",
    "\n",
    "input_data = [('C2.D4', 0.3), ('F#1.G#1.C4.F4.A4.C5', 0.3), ('D2.A2.D3.F3.A3.F4', 2), ('A3.B3', 1.25), ('A1.A2.A4.F5.A5', 0.3)]\n",
    "output_data = simplify_data(input_data)\n",
    "print(\"Input:\", input_data)\n",
    "print(\"Output:\", output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeefb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_chord_data = simplify_data(processed_chords)\n",
    "data_summary(cleaned_chord_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c2f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "plt.plot(sorted(Counter(cleaned_chord_data).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8863196",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_chord_data = []\n",
    "count = collections.Counter(cleaned_chord_data)\n",
    "\n",
    "for c in cleaned_chord_data:\n",
    "    if(count[c]>200):\n",
    "        pruned_chord_data.append(c)\n",
    "\n",
    "data_summary(pruned_chord_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f37ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_chord_data_to_midi_sample = get_music_midi_filename_from_chords(pruned_chord_data[1000:1020])\n",
    "FluidSynth().midi_to_audio(pruned_chord_data_to_midi_sample, \"music.wav\")\n",
    "\n",
    "Audio(\"music.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b11923",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(set(pruned_chord_data))\n",
    "mapping_from_id = dict(zip(range(len(c)), c))\n",
    "mapping_to_id = dict(zip(c, range(len(c))))\n",
    "chord_id_data = [mapping_to_id[ele] for ele in pruned_chord_data]\n",
    "\n",
    "data_summary(chord_id_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_network_input = []\n",
    "notes_network_output = []\n",
    "seq_len = 32\n",
    "n_notes = len(mapping_from_id)\n",
    "\n",
    "# create input sequences and the corresponding outputs\n",
    "for i in range(len(chord_id_data) - seq_len):\n",
    "    notes_sequence_in = chord_id_data[i:i + seq_len]\n",
    "    notes_sequence_out = chord_id_data[i + seq_len]\n",
    "    notes_network_input.append(notes_sequence_in)\n",
    "    notes_network_output.append(notes_sequence_out)\n",
    "\n",
    "n_patterns = len(notes_network_input)\n",
    "\n",
    "# reshape the input into a format compatible with LSTM layers\n",
    "network_input = np.reshape(notes_network_input, (n_patterns, seq_len, 1))/n_notes\n",
    "\n",
    "network_output = keras.utils.np_utils.to_categorical(notes_network_output, num_classes=n_notes)\n",
    "\n",
    "print(\"Input shape\", network_input.shape)\n",
    "print(\"Output shape\", network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "def create_network(X, y):\n",
    "    #Initialising the Model\n",
    "    model = Sequential()\n",
    "    #Adding layers\n",
    "    model.add(LSTM(512, input_shape=(32, 1), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    #Compiling the model for training  \n",
    "    opt = Adamax(learning_rate=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ad1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_network(network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4baedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(network_input, network_output, batch_size=256, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_music_id = chord_id_data[:32]\n",
    "n_notes = len(mapping_from_id)\n",
    "\n",
    "for i in range(100):\n",
    "    model_input = generated_music_id[-32:]\n",
    "    \n",
    "    model_input = np.reshape(model_input, (1,32,1))/n_notes\n",
    "    \n",
    "    model_output = model.predict(model_input)\n",
    "    model_output = model_output.argmax(axis=-1)[0]\n",
    "    \n",
    "    generated_music_id.append(model_output)\n",
    "    \n",
    "generated_music_id = generated_music_id[32:] # remove the random part of the song\n",
    "print(generated_music_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_music = [mapping_from_id[ele] for ele in generated_music_id]\n",
    "print(\"Generated music:\", generated_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd1a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_music_to_midi = get_music_midi_filename_from_chords(generated_music)\n",
    "FluidSynth().midi_to_audio(generated_music_to_midi, \"music.wav\")\n",
    "\n",
    "print(\"\\n\\nThe music we created using our AI üéâüéµ \\n\")\n",
    "Audio(\"music.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pedalboard import Pedalboard, Chorus, Reverb\n",
    "from pedalboard.io import AudioFile\n",
    "\n",
    "# Read in a whole audio file:\n",
    "with AudioFile('music.wav') as f:\n",
    "  audio = f.read(f.frames)\n",
    "  samplerate = f.samplerate\n",
    "\n",
    "# Make a Pedalboard object, containing multiple plugins:\n",
    "board = Pedalboard([Reverb(room_size=0.6)])\n",
    "\n",
    "# Run the audio through this pedalboard!\n",
    "effected = board(audio, samplerate)\n",
    "\n",
    "# Write the audio back as a wav file:\n",
    "with AudioFile('processed-music.wav', 'w', samplerate, effected.shape[0]) as f:\n",
    "  f.write(effected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nMusic after some reverb ü•µüå∂Ô∏è \\n\")\n",
    "Audio(\"processed-music.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf63ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
